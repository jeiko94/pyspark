{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ManipulaciónDatos\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+---------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|  CustomerName|        EmailAddress|                Item|Quantity|UnitPrice|TaxAmount|\n",
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+---------+\n",
      "|         SO43701|                   1|2019-07-01|   Christy Zhu|christy12@adventu...|Mountain-100 Silv...|       1|  3399.99| 271.9992|\n",
      "|         SO43704|                   1|2019-07-01|    Julio Ruiz|julio1@adventure-...|Mountain-100 Blac...|       1|  3374.99| 269.9992|\n",
      "|         SO43705|                   1|2019-07-01|     Curtis Lu|curtis9@adventure...|Mountain-100 Silv...|       1|  3399.99| 271.9992|\n",
      "|         SO43700|                   1|2019-07-01|  Ruben Prasad|ruben10@adventure...|  Road-650 Black, 62|       1| 699.0982|  55.9279|\n",
      "|         SO43703|                   1|2019-07-01|Albert Alvarez|albert7@adventure...|    Road-150 Red, 62|       1|  3578.27| 286.2616|\n",
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('sales.csv', header=True, sep=',',inferSchema=True)\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+----------------+--------------------+----------------+--------+---------+---------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|    CustomerName|        EmailAddress|            Item|Quantity|UnitPrice|TaxAmount|\n",
      "+----------------+--------------------+----------+----------------+--------------------+----------------+--------+---------+---------+\n",
      "|         SO43707|                   1|2019-07-02|      Emma Brown|emma3@adventure-w...|Road-150 Red, 48|       1|  3578.27| 286.2616|\n",
      "|         SO43711|                   1|2019-07-02|Courtney Edwards|courtney1@adventu...|Road-150 Red, 56|       1|  3578.27| 286.2616|\n",
      "|         SO43706|                   1|2019-07-02|    Edward Brown|edward26@adventur...|Road-150 Red, 48|       1|  3578.27| 286.2616|\n",
      "|         SO43708|                   1|2019-07-02|       Brad Deng|brad2@adventure-w...|Road-650 Red, 52|       1| 699.0982|  55.9279|\n",
      "|         SO43709|                   1|2019-07-02|       Martha Xu|martha12@adventur...|Road-150 Red, 52|       1|  3578.27| 286.2616|\n",
      "+----------------+--------------------+----------+----------------+--------------------+----------------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado = df_spark.filter(col('OrderDate') > '2019-07-01')\n",
    "df_filtrado.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seleccionar columnas específicas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------------+--------+---------+\n",
      "|SalesOrderNumber|  CustomerName|                Item|Quantity|UnitPrice|\n",
      "+----------------+--------------+--------------------+--------+---------+\n",
      "|         SO43701|   Christy Zhu|Mountain-100 Silv...|       1|  3399.99|\n",
      "|         SO43704|    Julio Ruiz|Mountain-100 Blac...|       1|  3374.99|\n",
      "|         SO43705|     Curtis Lu|Mountain-100 Silv...|       1|  3399.99|\n",
      "|         SO43700|  Ruben Prasad|  Road-650 Black, 62|       1| 699.0982|\n",
      "|         SO43703|Albert Alvarez|    Road-150 Red, 62|       1|  3578.27|\n",
      "+----------------+--------------+--------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar columnas específicas\n",
    "df_selected = df_spark.select('SalesOrderNumber', 'CustomerName', 'Item', 'Quantity', 'UnitPrice')\n",
    "df_selected.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear columnas calculadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------+\n",
      "|SalesOrderNumber|   Total|TotalConImpuesto|\n",
      "+----------------+--------+----------------+\n",
      "|         SO43701| 3399.99|       3671.9892|\n",
      "|         SO43704| 3374.99|       3644.9892|\n",
      "|         SO43705| 3399.99|       3671.9892|\n",
      "|         SO43700|699.0982|        755.0261|\n",
      "|         SO43703| 3578.27|       3864.5316|\n",
      "+----------------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_calculado = df_spark.withColumn('Total', col('Quantity') * col('UnitPrice')) \\\n",
    "    .withColumn('TotalConImpuesto', col('Total') + col('TaxAmount'))\n",
    "\n",
    "df_calculado.select('SalesOrderNumber', 'Total', 'TotalConImpuesto').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renombrar columnas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+--------------+--------------------+--------------------+--------+--------------+---------+\n",
      "|NumeroOrden|SalesOrderLineNumber|FechaOrden|       Cliente|        EmailAddress|            Producto|Cantidad|PrecioUnitario|TaxAmount|\n",
      "+-----------+--------------------+----------+--------------+--------------------+--------------------+--------+--------------+---------+\n",
      "|    SO43701|                   1|2019-07-01|   Christy Zhu|christy12@adventu...|Mountain-100 Silv...|       1|       3399.99| 271.9992|\n",
      "|    SO43704|                   1|2019-07-01|    Julio Ruiz|julio1@adventure-...|Mountain-100 Blac...|       1|       3374.99| 269.9992|\n",
      "|    SO43705|                   1|2019-07-01|     Curtis Lu|curtis9@adventure...|Mountain-100 Silv...|       1|       3399.99| 271.9992|\n",
      "|    SO43700|                   1|2019-07-01|  Ruben Prasad|ruben10@adventure...|  Road-650 Black, 62|       1|      699.0982|  55.9279|\n",
      "|    SO43703|                   1|2019-07-01|Albert Alvarez|albert7@adventure...|    Road-150 Red, 62|       1|       3578.27| 286.2616|\n",
      "+-----------+--------------------+----------+--------------+--------------------+--------------------+--------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renombrar columnas\n",
    "df_renamed = df_spark.withColumnRenamed('SalesOrderNumber', 'NumeroOrden') \\\n",
    "                     .withColumnRenamed('OrderDate', 'FechaOrden') \\\n",
    "                     .withColumnRenamed('CustomerName', 'Cliente') \\\n",
    "                     .withColumnRenamed('Item', 'Producto') \\\n",
    "                     .withColumnRenamed('Quantity', 'Cantidad') \\\n",
    "                     .withColumnRenamed('UnitPrice', 'PrecioUnitario')\n",
    "\n",
    "df_renamed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpieza básica (duplicados y valores faltantes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|SalesOrderNumber|Cantidad|\n",
      "+----------------+--------+\n",
      "|         SO51539|       3|\n",
      "|         SO51654|       4|\n",
      "|         SO52871|       2|\n",
      "|         SO53123|       2|\n",
      "|         SO53163|       2|\n",
      "|         SO53404|       3|\n",
      "|         SO53414|       2|\n",
      "|         SO53883|       3|\n",
      "|         SO53987|       2|\n",
      "|         SO54100|       2|\n",
      "|         SO54042|       7|\n",
      "|         SO54611|       2|\n",
      "|         SO54999|       2|\n",
      "|         SO55549|       2|\n",
      "|         SO56348|       3|\n",
      "|         SO56522|       3|\n",
      "|         SO56596|       3|\n",
      "|         SO56614|       3|\n",
      "|         SO56935|       2|\n",
      "|         SO57196|       3|\n",
      "+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identificar duplicados\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_duplicates = df_spark.groupBy('SalesOrderNumber') \\\n",
    "    .agg(F.count('*').alias('Cantidad')) \\\n",
    "    .filter(F.col('Cantidad') > 1)\n",
    "\n",
    "df_duplicates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+----------------+--------------------+--------------------+--------+---------+---------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|    CustomerName|        EmailAddress|                Item|Quantity|UnitPrice|TaxAmount|\n",
      "+----------------+--------------------+----------+----------------+--------------------+--------------------+--------+---------+---------+\n",
      "|         SO43697|                   1|2019-07-01|     Cole Watson|cole1@adventure-w...|    Road-150 Red, 62|       1|  3578.27| 286.2616|\n",
      "|         SO43698|                   1|2019-07-01|Rachael Martinez|rachael16@adventu...|Mountain-100 Silv...|       1|  3399.99| 271.9992|\n",
      "|         SO43699|                   1|2019-07-01|   Sydney Wright|sydney61@adventur...|Mountain-100 Silv...|       1|  3399.99| 271.9992|\n",
      "|         SO43700|                   1|2019-07-01|    Ruben Prasad|ruben10@adventure...|  Road-650 Black, 62|       1| 699.0982|  55.9279|\n",
      "|         SO43701|                   1|2019-07-01|     Christy Zhu|christy12@adventu...|Mountain-100 Silv...|       1|  3399.99| 271.9992|\n",
      "+----------------+--------------------+----------+----------------+--------------------+--------------------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminar duplicados (manteniendo el menor SalesOrderLineNumber)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy('SalesOrderNumber').orderBy('SalesOrderLineNumber')\n",
    "\n",
    "df_no_duplicates = df_spark.withColumn('row_num', row_number().over(windowSpec)) \\\n",
    "    .filter(col('row_num') == 1) \\\n",
    "    .drop('row_num')\n",
    "# Manejar valores faltantes (si existen)\n",
    "df_no_duplicates = df_no_duplicates.na.drop()\n",
    "df_no_duplicates.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|SalesOrderNumber|Cantidad|\n",
      "+----------------+--------+\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_duplicates = df_no_duplicates.groupBy('SalesOrderNumber') \\\n",
    "    .agg(F.count('*').alias('Cantidad')) \\\n",
    "    .filter(F.col('Cantidad') > 1)\n",
    "\n",
    "df_duplicates.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Algoritmos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
